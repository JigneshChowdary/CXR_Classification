from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Input, ZeroPadding2D, GlobalAveragePooling2D, Reshape, Dense, Multiply
from tensorflow.keras.models import Model



def DilatedInceptionModule(inputs, numFilters = 64): 
    tower_0 = Convolution2D(numFilters, (1,1), padding='same', dilation_rate = (1,1), kernel_initializer = 'he_normal', activation='relu')(inputs)
    tower_0 = BatchNormalization()(tower_0)
    
    
    tower_1 = Convolution2D(numFilters, (1,1), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(inputs)
    tower_1 = Convolution2D(numFilters, (3,3), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(tower_1)
    tower_1 = BatchNormalization()(tower_1)
    
    
    tower_2 = Convolution2D(numFilters, (1,1), padding='same', dilation_rate = (3,3), kernel_initializer = 'he_normal')(inputs)
    tower_2 = Convolution2D(numFilters, (3,3), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(tower_2)
    tower_2 = BatchNormalization()(tower_2)
    tower_2 = Convolution2D(numFilters, (3,3), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(tower_2)
    tower_2 = BatchNormalization()(tower_2)
    
    tower_3 = Convolution2D(numFilters, (1,1), padding='same', dilation_rate = (3,3), kernel_initializer = 'he_normal')(inputs)
    tower_3 = Convolution2D(numFilters, (3,3), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(tower_3)
    tower_3 = BatchNormalization()(tower_3)
    tower_3 = Convolution2D(numFilters, (3,3), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(tower_3)
    tower_3 = BatchNormalization()(tower_3)
    tower_3 = Convolution2D(numFilters, (5,5), padding='same', dilation_rate = (2,2), kernel_initializer = 'he_normal', activation='relu')(tower_3)
    tower_3 = BatchNormalization()(tower_3)

    
    dilated_inception_module2 = concatenate([tower_0, tower_1, tower_2, tower_3], axis = 3)
    sop = Conv2D(numFilters, (1,1), padding="same", strides=1, activation='relu')(dilated_inception_module2)
    

    dilated_inception_module = concatenate([sop, inputs], axis = 3)
    
   
  
    
   
    return dilated_inception_module


    

n_labels = 1
numFilters = 64
output_mode="softmax"


def build_resunet(input_shape):
    inputs = Input(input_shape)

    """ Encoder 1 """
    conv1 = DilatedInceptionModule(inputs, numFilters)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    
    conv2 = DilatedInceptionModule(pool1, 2*numFilters)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    
    conv3 = DilatedInceptionModule(pool2, 4*numFilters)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    
    conv4 = DilatedInceptionModule(pool3, 8*numFilters)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    """ Bridge """
    conv5 = DilatedInceptionModule(pool4,16*numFilters)


    """ Decoder 1, 2, 3 """
    up6 = UpSampling2D(size=(2,2))(conv5)
    up6 = DilatedInceptionModule(up6, 8*numFilters)
    merge6 = concatenate([conv4,up6],axis=3)
    
    up7 = UpSampling2D(size=(2,2))(merge6)
    up7 = DilatedInceptionModule(up7, 4*numFilters)
    merge7 = concatenate([conv3,up7],axis=3)
    
    up8 = UpSampling2D(size=(2,2))(merge7)
    up8 = DilatedInceptionModule(up8, 2*numFilters)
    merge8 = concatenate([conv2,up8],axis=3)
    
    up9 = UpSampling2D(size=(2,2))(merge8)
    up9 = DilatedInceptionModule(up9, numFilters)
    merge9 = concatenate([conv1,up9],axis=3)
  
    """ Classifier """
    outputs = Conv2D(1, 1, padding="same", activation="sigmoid")(up9)

    """ Model """
    model = Model(inputs, outputs)
    return model

if __name__ == "__main__":
    model = build_resunet((224, 224, 3))
    model.summary()

def step_decay(epoch):
	initial_lrate = 0.0005
	drop = 40
	epochs_drop = 20.0
	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
	return lrate
  
model.compile(
  loss='dice_loss',
  optimizer='adam',
  metrics=['dice_score']
)

learning_rate = tf.keras.callbacks.LearningRateScheduler(step_decay)



def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=200, BATCH_SIZE=16):
    model = None
    model = cnn_model(IMAGE_SIZE, 2)
    results = model.fit(t_x, t_y, epochs=200, batch_size=16, callbacks=[learning_rate], 
              verbose=1, validation_split=0.1)  
    print("Val Score: ", model.evaluate(val_x, val_y))
    return results

n_folds=5
epochs=200
batch_size=16

#save the model history in a list after fitting so that we can plot later
model_history = [] 

for i in range(n_folds):
    print("Training on Fold: ",i+1)
    t_x, val_x, t_y, val_y = train_test_split(train_x, train_y, test_size=0.1, 
                                               random_state = np.random.randint(1,1000, 1)[0])
    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))
    print("======="*12, end="\n\n\n")
